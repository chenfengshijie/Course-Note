# 监督学习

## 感知机

- 概念：
  - 感知机模型的基本形式是：

    $f(x) = sign(w \cdot x + b)$

    其中，$x$ 是输入样本的特征向量，$w$ 是权值向量，$b$ 是偏置量，$w \cdot x$ 表示向量 $w$ 和 $x$ 的点积。$sign$ 函数表示符号函数，当输入大于 0 时输出 1，否则输出 -1。

- 要求模型必须线性可分

## K近邻

- 基本思想：是对于一个新的输入样本，在训练数据集中找出与之最邻近的k个样本，并将其预测结果作为该样本的输出。
- 步骤
  1. 计算测试样本与训练样本集中每个样本的距离；
  2. 选取距离最近的k个训练样本；
    对于分类问题，采用投票法，即将k个样本中出现最多的类别作为预测结果；对于回归问题，采用平均值，即将k个样本的输出值的平均值作为预测结果。

- 选取最近的k个样本一般采用kd树来进行实现。
    > kd树采取方差最大的那一变量（的中位数）进行分割
    
    > kd树的查询首先寻找到该点所在的子节点的部分。然后逐渐向上递归比较父节点和父节点的另一个子节点是否在某个领域（当前的最小距离）内具有交际。

## 朴素贝叶斯

- 假设每个特征之间相互独立。即$P(X_1,X_2,X_3,...,X_n|Y)=P(X_1|Y)*P(X_2|Y)*...*P(X_n|Y)$
- 后验概率最大化，无论是采用极大似然估计或者贝叶斯估计，都可以推导出相应的公式。
- 假设有 $n$ 个特征和 $m$ 个类别，我们需要分类一个新的样本 $x$，其中 $x_i$ 表示第 $i$ 个特征的取值。根据贝叶斯定理，可以计算出给定样本 $x$ 属于第 $j$ 个类别的后验概率 $P(C_j | x)$，即：

$$
P(C_j|X) = \frac{P(X|C_j)P(C_j)}{P(X)}
$$
​
 
其中，$P(C_j)$ 表示类别 $j$ 在训练集中的先验概率，$P(x | C_j)$ 表示样本 $x$ 在给定类别 $j$ 的条件下的概率密度函数（通常假设为高斯分布,或者直接使用频率代替概率），$P(x)$ 表示样本 $x$ 在所有类别下的概率。由于分母 $P(x)$ 对于所有类别来说都是相同的，因此可以省略，只需要计算分子即可。此时，$P(C_j | x)$ 可以看作样本 $x$ 属于类别 $j$ 的“置信度”，将样本分配给概率最大的类别即可。

## 决策树

- 分为三个步骤：特征选择、树的生成和剪枝。
- 需要了解下面几个概念：
$$\text{熵：}H(Y) = -\sum_{y \in Y} p(y) \log_2 p(y)$$
$$H(Y|X)=\sum_{i=1}^{n}p_iH(Y|X=x_i)$$
$$\text{信息增益：IG}(X, Y) = H(Y) - H(Y|X)$$



$$\text{信息增益比：IGR}(X, Y) = \frac{\text{IG}(X, Y)}{H(X)}$$


$$\text{基尼指数：Gini}(Y) = \sum_{i=1}^{|Y|} \sum_{j\neq i} p_i p_j = 1 - \sum_{i=1}^{|Y|} p_i^2$$

$$\text{Gini}(X,Y) = \sum_{D_i=1}^{|D|}p_i\text{Gini}(D_i)$$

- 不同的决策树算法就是基于上述不同的指标来进行特征的选择。

- 剪枝算法：首先定义一个损失函数$L(T)=\sum_{t=1}^{|T|}N_tH_t(T)+\alpha|T|,其中T为子节点，N_t为子节点的样本点数量。$对于决策树每一个子节点，如果多个子节点的损失大于父节点将他们吸收的损失，那么父节点就合并所有的子节点，并向上计算，可以通过递归（或者非递归）的动态规划进行解决。

## logitics和最大熵模型

类似感知机，只是将最终的函数由sign改为了logistics。

## 支持向量机

首先了解以下概念：
  1. 函数间隔.$y|wx+b|$
  2. 几何间隔.$y\frac{|wx+b|}{|w|}$
  3. 线性支持向量机就是要最大几何间隔。
  4. 拉格朗日对偶原理和拉格朗日乘子法。[拉格朗日对偶](https://zhuanlan.zhihu.com/p/114574438)
  5. 支持向量
  6. 合页函数最优化求解和支持向量的原问题的等价性
  7. SMO启发式方法

## AdaBoost



