# 最小方差无偏估计

$$
无偏估计：
\\
\text{for any parameters }\\
E(\hat \theta) = \theta 
$$


# 最大似然估计
根据贝叶斯定理，$P(\theta|D) = \frac{P(D|\theta)*P(\theta)}{P(P(D))}$,由于没有关于$\theta$的先验信息，所以忽略掉$P(\theta),P(D)$，最终只需要最大化$P(D|\theta)$

- 要求每次样本独立
- 没有隐参数


就是最小化
$$
 \hat \theta = \argmax P(\bold x| \theta) 
$$


# 最大后验估计

就是最小化
$$
 \hat \theta = \argmax P(\bold x| \theta) * P(\theta)
$$

# 贝叶斯估计

[贝叶斯估计原理](https://blog.csdn.net/zengxiantao1994/article/details/72889732)

[例题](https://blog.csdn.net/fjswcjswzy/article/details/104654929)

# 期望最大化算法（EM)

往往用于具有隐参数模型的收敛，如混合高斯模型、k-means模型。

[推导](https://www.cnblogs.com/pinard/p/6912636.html)
[简单案例](https://zhuanlan.zhihu.com/p/78311644#:~:text=EM%20%E7%AE%97%E6%B3%95%EF%BC%8C%E5%85%A8%E7%A7%B0%20Expectation,Maximization%20Algorithm%E3%80%82%20%E6%9C%9F%E6%9C%9B%E6%9C%80%E5%A4%A7%E7%AE%97%E6%B3%95%E6%98%AF%E4%B8%80%E7%A7%8D%E8%BF%AD%E4%BB%A3%E7%AE%97%E6%B3%95%EF%BC%8C%E7%94%A8%E4%BA%8E%E5%90%AB%E6%9C%89%E9%9A%90%E5%8F%98%E9%87%8F%EF%BC%88Hidden%20Variable%EF%BC%89%E7%9A%84%E6%A6%82%E7%8E%87%E5%8F%82%E6%95%B0%E6%A8%A1%E5%9E%8B%E7%9A%84%E6%9C%80%E5%A4%A7%E4%BC%BC%E7%84%B6%E4%BC%B0%E8%AE%A1%E6%88%96%E6%9E%81%E5%A4%A7%E5%90%8E%E9%AA%8C%E6%A6%82%E7%8E%87%E4%BC%B0%E8%AE%A1%E3%80%82)


